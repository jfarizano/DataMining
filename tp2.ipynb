{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2 - Tópicos de Minería de Datos\n",
    "### Juan Ignacio Farizano\n",
    "\n",
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "randomForest 4.7-1.1\n",
      "\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import iniciales\n",
    "library(randomForest)\n",
    "library(kernlab)\n",
    "library(MASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Código proporcionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "#general forward greedy selection function\n",
    "#input:\n",
    "# x,y inputs and targets\n",
    "# method is an external function that estimates classification error with a given model\n",
    "# ... parameters for method\n",
    "#output:\n",
    "#ordered.names.list <- nombre de las variables ordenadas de la mas importante a la menos\n",
    "#ordered.features.list <-numero de orden inicial de las variables, con el mismo orden\n",
    "#importance <- importancia de cada variables en el mismo orden\n",
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "forward.ranking <- function(x,y,method,verbosity=0,... ) {\n",
    "\n",
    "\tmax.feat<-dim(x)[2] # Número total de variables\n",
    "\tnum.feat<-1 # Número de variables elegidas en el momento\n",
    "\tlist.feat<-1:max.feat\n",
    "\n",
    "\t# ranking inicial: elijo la variable con menor error de prediccion\n",
    "  x.train<-matrix(0,dim(x)[1],1) # Variable de entrenamiento con solo las columnas elegidas\n",
    "\tclass.error<-double(max.feat) # Error de cada variable\n",
    "\t# para cada i, creo el dataset con esa variable sola, entreno un modelo\n",
    "\t# y le mido el error, que lo guardo en class.error[i]\n",
    "\tfor(i in 1:max.feat) {\n",
    "\t\tx.train[,1]<-x[,i]\n",
    "\t\tclass.error[i] <- do.call(method, c(list(x.train, y), list(...)) )\n",
    "\t}\n",
    "\t# guardo la variable con minimo error como primera. Guardo una lista\n",
    "\t# keep.feat con las que me quedan para seguir eligiendo.\n",
    "\tlist.feat[1]<-which.min(class.error)\n",
    "\tkeep.feat<-sort(class.error,decreasing=FALSE,index=T)$ix[-1]\n",
    "\t# armo un dataset con las variables que ya elegi, para ir agregando en cada paso.\n",
    "\tx.prev<-x.train[,1]<-x[,list.feat[1]]\n",
    "\n",
    "\tif(verbosity>1) cat(\"\\nFirst chosen feature: \",colnames(x)[list.feat[1]],\"\\n\")\n",
    "\n",
    "  # loop principal. A cada paso agrego todas las variables disponibles,\n",
    "\t# de a una, le mido el error y me quedo con la de minimo error.\n",
    "\t# Hasta llegar a meter todas.\n",
    "\twhile(num.feat < max.feat) {\n",
    "    # class.error guarda el error de cada modelo. Son max.feat-num.feat modelos.\n",
    "\t\tclass.error<-double(max.feat - num.feat)\n",
    "\t\t# para cada variable que me queda, la agrego al dataset del paso anterior,\n",
    "\t\t# entreno el modelo y le mido el error.\n",
    "\t\tfor(i in 1:(max.feat-num.feat)){\n",
    "\t\t\tx.train<-cbind(x.prev,x[,keep.feat[i]])\n",
    "\t\t\tclass.error[i] <- do.call(method, c(list(x.train, y), list(...)) )\n",
    "\t\t}\n",
    "\t\tif(verbosity>2) cat(\"\\nFeatures:\\n\",keep.feat,\"\\nErrors:\\n\",class.error)\n",
    "\t\t# me quedo con el modelo de minimo error, guardo ese feature en la lista\n",
    "\t\t# de las elegidas, lo saco de la lista de las que quedan, y actualizo\n",
    "\t\t# el dataset de partida de la iteracion.\n",
    "\t\tbest.index<-which.min(class.error)\n",
    "\t\tlist.feat[num.feat+1]<-keep.feat[best.index]\n",
    "\t\tif(verbosity>1) cat(\"\\n---------\\nStep \",1+num.feat,\"\\nChosen feature:\",colnames(x)[keep.feat[best.index]])\n",
    "\n",
    "\t\tkeep.feat<-keep.feat[-best.index]\n",
    "\t\tif(verbosity>2) cat(\"\\nNew search list: \",keep.feat)\n",
    "\t\tnum.feat<-num.feat+1\n",
    "\t\tx.prev<-x[,list.feat[1:num.feat]]\n",
    "\t}\n",
    "\n",
    "\n",
    "\tsearch.names<-colnames(x)[list.feat]\n",
    "\t# le asigno a cada feature una importacia proporcional al orden en que\n",
    "\t# lo seleccionamos\n",
    "\timp<-(max.feat:1)/max.feat\n",
    "\tnames(imp)<-search.names\n",
    "\n",
    "\tif(verbosity>1) {\n",
    "\t\tcat(\"\\n---------\\nFinal ranking \",num.feat,\" features.\")\n",
    "\t\tcat(\"\\nFeatures: \",search.names,\"\\n\")\n",
    "\t}\n",
    "\n",
    " \treturn( list(ordered.names.list=search.names,ordered.features.list=list.feat,importance=imp) )\n",
    "}\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "#random forest error estimation (OOB) for greedy search\n",
    "#---------------------------------------------------------------------------\n",
    "rf.est <- function(x.train,y,equalize.classes=TRUE,tot.trees=500,mtry=0) {\n",
    "\tif(mtry<1) mtry<-floor(sqrt(dim(x.train)[2]))\n",
    "\tprop.samples<-table(y)\n",
    "\tif(equalize.classes) prop.samples<-rep(min(prop.samples),length(prop.samples))\n",
    "\treturn( randomForest(x.train,y,mtry=mtry,ntree=tot.trees,sampsize=prop.samples)$err.rate[tot.trees] )\n",
    "}\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "#LDA error estimation (LOO) for greedy search\n",
    "#---------------------------------------------------------------------------\n",
    "lda.est <- function(x.train,y) {\n",
    "\tm.lda <- lda(x.train,y,CV=TRUE)\n",
    "\treturn(error.rate( y , m.lda$class))\n",
    "}\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "#SVM error estimation (internal CV) for greedy search\n",
    "#---------------------------------------------------------------------------\n",
    "svm.est <- function(x.train,y,type=\"C-svc\",kernel=\"vanilladot\",C=1,cross = 4)\n",
    "{\n",
    "\treturn ( ksvm(x.train, y, type=type,kernel=kernel,C=C,cross = cross)@cross )\n",
    "}\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "#random forest ranking method for rfe.\n",
    "#---------------------------------------------------------------------------\n",
    "imp.rf <- function(x.train,y,equalize.classes=TRUE,tot.trees=500,mtry=0)\n",
    "{\n",
    "\tif(mtry<1) mtry<-floor(sqrt(dim(x.train)[2]))\n",
    "\tprop.samples<-table(y)\n",
    "\tif(equalize.classes) prop.samples<-rep(min(prop.samples),length(prop.samples))\n",
    "\t\n",
    "\tm.rf<-randomForest(x.train,y,ntree=tot.trees,mtry=mtry,sampsize=prop.samples,importance=TRUE)\n",
    "\timp.mat<-importance(m.rf)\n",
    "\timp.col<-dim(imp.mat)[2]-1\n",
    "\trank.list<-sort(imp.mat[,imp.col],decreasing=FALSE,index=T)\n",
    "\treturn(list(feats=rank.list$ix,imp=rank.list$x))\n",
    "}\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "#linear svm ranking method for rfe. Using kernlab. Multiclass\n",
    "#---------------------------------------------------------------------------\n",
    "imp.linsvm <- function(x.train,y,C=100)\n",
    "{\n",
    "\tnum.feat<-dim(x.train)[2]\n",
    "\ttot.problems<-nlevels(y)*(nlevels(y)-1)/2\n",
    "\n",
    "\tm.svm <- ksvm(as.matrix(x.train), y, type=\"C-svc\",kernel=\"vanilladot\",C=C)\n",
    "\n",
    "\tw<-rep(0.0,num.feat)\n",
    "\tfor(i in 1:tot.problems) for(feat in 1:num.feat)\n",
    "\t\tw[feat]<-w[feat]+abs(m.svm@coef[[i]] %*% m.svm@xmatrix[[i]][,feat])\n",
    "\trank.list<-sort(w,decreasing=FALSE,index=T)\n",
    "\treturn(list(feats=rank.list$ix,imp=rank.list$x))\n",
    "}\n",
    "\n",
    "error.rate <- function(dataA, dataB) sum( dataA != dataB ) / length(dataB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#hacer una funcion que cree datos, 2 clases (-1 y 1,n puntos de cada una), d dimensiones, de ruido uniforme [-1,1], con la clase al azar\n",
    "crea.ruido.unif<-function(n=100,d=2){\n",
    "x<-runif(2*n*d,min=-1)\t#genero los datos\n",
    "dim(x)<-c(2*n,d)\n",
    "return(cbind(as.data.frame(x),y=factor(rep(c(-1,1),each=n))))\t#le agrego la clase\n",
    "}\n",
    "\n",
    "#datosA\n",
    "d<-10\n",
    "n<-1000\n",
    "datos<-crea.ruido.unif(n=n,d=d)\n",
    "\n",
    "#tomar 50% de los datos al azar, y hacer que la clase sea el signo de la 8 variable\n",
    "shuffle<-sample(1:dim(datos)[1])\n",
    "sub<-shuffle[1:dim(datos)[1]*0.5]\n",
    "datos[sub,d+1]<-sign(datos[sub,8])\n",
    "#tomar 20% de los datos al azar (fuera de los anteriores), y hacer que la clase sea el signo de la 6 variable\n",
    "sub<-shuffle[(dim(datos)[1]*0.5):(dim(datos)[1]*0.7)]\n",
    "datos[sub,d+1]<-sign(datos[sub,6])\n",
    "#tomar 10% de los datos al azar, y hacer que la clase sea el signo de la 4 variable\n",
    "sub<-shuffle[(dim(datos)[1]*0.7):(dim(datos)[1]*0.8)]\n",
    "datos[sub,d+1]<-sign(datos[sub,4])\n",
    "#tomar 5% de los datos al azar, y hacer que la clase sea el signo de la 2 variable\n",
    "sub<-shuffle[(dim(datos)[1]*0.8):(dim(datos)[1]*0.85)]\n",
    "datos[sub,d+1]<-sign(datos[sub,2])\n",
    "datos[,d+1]<-factor(datos[,d+1])\n",
    "\n",
    "datosA<-datos\n",
    "\n",
    "#datosB\n",
    "#generar n=100,d=8\n",
    "d<-8\n",
    "n<-1000\n",
    "datos<-crea.ruido.unif(n=n,d=d)\n",
    "#hacer que la clase sea el xor de las 2 primeras variables (es usando el signo)\n",
    "datos[,d+1]<-sign(datos[,1]*datos[,2])\n",
    "#hacer que las variables 3 y 4 tengan un 50% de correlacion con la clase\n",
    "shuffle<-sample(1:dim(datos)[1])\n",
    "sub<-shuffle[1:dim(datos)[1]*0.5]\n",
    "datos[sub,3]<-abs(datos[sub,3])*datos[sub,d+1]\n",
    "shuffle<-sample(1:dim(datos)[1])\n",
    "sub<-shuffle[1:dim(datos)[1]*0.5]\n",
    "datos[sub,4]<-abs(datos[sub,4])*datos[sub,d+1]\n",
    "datos[,d+1]<-factor(datos[,d+1])\n",
    "\n",
    "datosB<-datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "backward.ranking <- function(x,y,method,verbosity=0,... ) {\n",
    "\tmax.feat<-dim(x)[2] # Número total de variables\n",
    "\tnum.feat <- max.feat\n",
    "\tdiscarded.feat<-1:max.feat\n",
    "\tkeep.feat <- discarded.feat\n",
    "\n",
    "  x.train<-matrix(0,dim(x)[1],1) # Variable de entrenamiento con solo las columnas elegidas\n",
    "\tx.prev <- x\n",
    "\n",
    "  # loop principal. A cada paso quito todas las variables disponibles,\n",
    "\t# de a una, le mido el error y descarto con la de mínimo error.\n",
    "\t# hasta no quedarme con ninguna.\n",
    "\twhile(num.feat > 1) {\n",
    "    # class.error guarda el error de cada modelo. Son num.feat modelos.\n",
    "\t\tclass.error<-double(num.feat)\n",
    "\t\t# para cada variable que me queda, la saco del dataset del paso anterior,\n",
    "\t\t# entreno el modelo y le mido el error.\n",
    "\t\tfor(i in 1:num.feat){\n",
    "\t\t\tx.train<-x.prev[,-i, drop=FALSE]\n",
    "\t\t\tclass.error[i] <- do.call(method, c(list(x.train, y), list(...)) )\n",
    "\t\t}\n",
    "\t\tif(verbosity>2) cat(\"\\nFeatures:\\n\",keep.feat,\"\\nErrors:\\n\",class.error)\n",
    "\t\t# me quedo con el modelo de minimo error, guardo ese feature en la lista\n",
    "\t\t# de las descartadas, lo saco de la lista de las que quedan, y actualizo\n",
    "\t\t# el dataset de partida de la iteracion.\n",
    "\t\tworst.index<-which.min(class.error)\n",
    "\t\tdiscarded.feat[(max.feat - num.feat) + 1]<-keep.feat[worst.index]\n",
    "\t\tif(verbosity>1) cat(\"\\n---------\\nStep \",(max.feat - num.feat) + 1,\"\\nDiscarded feature: \",colnames(x)[keep.feat[worst.index]])\n",
    "\n",
    "\t\tkeep.feat<-keep.feat[-worst.index]\n",
    "\t\tif(verbosity>2) cat(\"\\nNew search list: \",keep.feat)\n",
    "\t\tnum.feat<-num.feat-1\n",
    "\t\tx.prev<-x[,keep.feat[1:num.feat], drop=FALSE] #TODO esta línea\n",
    "\t}\n",
    "\n",
    "\t# la única variable que quedó sin descartar es la más importante\n",
    "\tdiscarded.feat[max.feat]<-keep.feat[1]\n",
    "\tcat(\"\\nLast discarded feature: \",colnames(x)[keep.feat[1]])\n",
    "\n",
    "\t# las variables menos importantes se descartaron primero -> las mas importantes\n",
    "\t# están al final, doy vuelta la lista\n",
    "\tlist.feat <- rev(discarded.feat)\n",
    "\tsearch.names<-colnames(x)[list.feat]\n",
    "\t# le asigno a cada feature una importacia proporcional al orden en que\n",
    "\t# lo seleccionamos\n",
    "\timp<-(max.feat:1)/max.feat\n",
    "\tnames(imp)<-search.names\n",
    "\n",
    "\tif(verbosity>1) {\n",
    "\t\tcat(\"\\n---------\\nFinal ranking \",max.feat,\" features.\")\n",
    "\t\tcat(\"\\nFeatures: \",search.names,\"\\n\")\n",
    "\t}\n",
    "\n",
    " \treturn( list(ordered.names.list=search.names,ordered.features.list=list.feat,importance=imp) )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "kruskalwallis.ranking <- function(x, y, verbosity=0,...) {\n",
    "  num.feat <- dim(x)[2]\n",
    "  list.feat <- 1:num.feat\n",
    "  class.statistic <- double(num.feat)\n",
    "\n",
    "  for (i in 1:num.feat){\n",
    "    x.test <- iris[,i]\n",
    "    class.statistic[i] <- kruskal.test(x.test,y)$statistic\n",
    "    if (verbosity > 1) {\n",
    "      cat(\"\\n---------\\nFeature \",i,colnames(x)[i],\"\\nKruskal-Wallis chi-squared:\",class.statistic[i])\n",
    "    }\n",
    "  }\n",
    "\n",
    "  list.feat <- sort(class.statistic,decreasing=TRUE,index=T)$ix\n",
    "  search.names <- colnames(x)[list.feat]\n",
    "\n",
    "  # le asigno a cada feature una importacia proporcional al orden en que\n",
    "\t# lo seleccionamos\n",
    "\timp <- (num.feat:1)/num.feat\n",
    "\tnames(imp) <- search.names\n",
    "\n",
    "  if (verbosity > 0) {\n",
    "\t\tcat(\"\\n---------\\nFinal ranking \",num.feat,\" features.\")\n",
    "\t\tcat(\"\\nFeatures: \",search.names,\"\\n\")\n",
    "\t}\n",
    "\n",
    "  return( list(ordered.names.list=search.names,ordered.features.list=list.feat,importance=imp) )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rfe.ranking <- function(x,y,method,verbosity=0,... ) {\n",
    "\tnum.feat <- dim(x)[2]\n",
    "\tdiscarded.feat <- 1:num.feat\n",
    "\tkeep.feat <- discarded.feat\n",
    "\n",
    "\tfor (i in 1:num.feat) {\n",
    "\t\tx.train <- as.matrix(x[,keep.feat])\n",
    "\t\trankings <- do.call(method, c(list(x.train, y), list(...)) )\n",
    "\t\tworst.index <- rankings$feats[1]\n",
    "\t\tif(verbosity>1) cat(\"\\n---------\\nStep \",i,\"\\nDiscarded feature: \",colnames(x)[keep.feat[worst.index]],\"\\n\")\n",
    "\t\tdiscarded.feat[i] <- keep.feat[worst.index]\n",
    "\t\tkeep.feat <- keep.feat[-worst.index]\n",
    "\t}\n",
    "\n",
    "\n",
    "\t# las variables menos importantes se descartaron primero -> las mas importantes\n",
    "\t# están al final, doy vuelta la lista\n",
    "\tlist.feat <- rev(discarded.feat)\n",
    "\tsearch.names<-colnames(x)[list.feat]\n",
    "\t# le asigno a cada feature una importacia proporcional al orden en que\n",
    "\t# lo seleccionamos\n",
    "\timp<-(num.feat:1)/num.feat\n",
    "\tnames(imp)<-search.names\n",
    "\n",
    "\tif(verbosity>1) {\n",
    "\t\tcat(\"\\n---------\\nFinal ranking \",num.feat,\" features.\")\n",
    "\t\tcat(\"\\nFeatures: \",search.names,\"\\n\")\n",
    "\t}\n",
    "\n",
    " \treturn( list(ordered.names.list=search.names,ordered.features.list=list.feat,importance=imp) )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Forward ranking random forest --\n",
      "\n",
      "First chosen feature:  Petal.Width \n",
      "\n",
      "---------\n",
      "Step  2 \n",
      "Chosen feature: Petal.Length\n",
      "---------\n",
      "Step  3 \n",
      "Chosen feature: Sepal.Width\n",
      "---------\n",
      "Step  4 \n",
      "Chosen feature: Sepal.Length\n",
      "---------\n",
      "Final ranking  4  features.\n",
      "Features:  Petal.Width Petal.Length Sepal.Width Sepal.Length \n",
      "\n",
      "-- Forward ranking lda --\n",
      "\n",
      "First chosen feature:  Petal.Width \n",
      "\n",
      "---------\n",
      "Step  2 \n",
      "Chosen feature: Petal.Length\n",
      "---------\n",
      "Step  3 \n",
      "Chosen feature: Sepal.Length\n",
      "---------\n",
      "Step  4 \n",
      "Chosen feature: Sepal.Width\n",
      "---------\n",
      "Final ranking  4  features.\n",
      "Features:  Petal.Width Petal.Length Sepal.Length Sepal.Width \n",
      "\n",
      "-- Backward ranking random forest --\n",
      "\n",
      "---------\n",
      "Step  1 \n",
      "Discarded feature:  Sepal.Width\n",
      "---------\n",
      "Step  2 \n",
      "Discarded feature:  Sepal.Length\n",
      "---------\n",
      "Step  3 \n",
      "Discarded feature:  Petal.Length\n",
      "Last discarded feature:  Petal.Width\n",
      "---------\n",
      "Final ranking  4  features.\n",
      "Features:  Petal.Width Petal.Length Sepal.Length Sepal.Width \n",
      "\n",
      "-- Backward ranking lda --\n",
      "\n",
      "---------\n",
      "Step  1 \n",
      "Discarded feature:  Sepal.Width\n",
      "---------\n",
      "Step  2 \n",
      "Discarded feature:  Sepal.Length\n",
      "---------\n",
      "Step  3 \n",
      "Discarded feature:  Petal.Length\n",
      "Last discarded feature:  Petal.Width\n",
      "---------\n",
      "Final ranking  4  features.\n",
      "Features:  Petal.Width Petal.Length Sepal.Length Sepal.Width \n",
      "\n",
      "-- Kruskal-Wallis --\n",
      "\n",
      "---------\n",
      "Feature  1 Sepal.Length \n",
      "Kruskal-Wallis chi-squared: 96.93744\n",
      "---------\n",
      "Feature  2 Sepal.Width \n",
      "Kruskal-Wallis chi-squared: 63.57115\n",
      "---------\n",
      "Feature  3 Petal.Length \n",
      "Kruskal-Wallis chi-squared: 130.411\n",
      "---------\n",
      "Feature  4 Petal.Width \n",
      "Kruskal-Wallis chi-squared: 131.1854\n",
      "---------\n",
      "Final ranking  4  features.\n",
      "Features:  Petal.Width Petal.Length Sepal.Length Sepal.Width \n",
      "\n",
      "-- RFE linea random forest --\n",
      "\n",
      "---------\n",
      "Step  1 \n",
      "Discarded feature:  Sepal.Width \n",
      "\n",
      "---------\n",
      "Step  2 \n",
      "Discarded feature:  Sepal.Length \n",
      "\n",
      "---------\n",
      "Step  3 \n",
      "Discarded feature:  Petal.Width \n",
      "\n",
      "---------\n",
      "Step  4 \n",
      "Discarded feature:  Petal.Length \n",
      "\n",
      "---------\n",
      "Final ranking  4  features.\n",
      "Features:  Petal.Length Petal.Width Sepal.Length Sepal.Width \n",
      "\n",
      "-- RFE linear SVM --\n",
      " Setting default kernel parameters  \n",
      "\n",
      "---------\n",
      "Step  1 \n",
      "Discarded feature:  Sepal.Width \n",
      " Setting default kernel parameters  \n",
      "\n",
      "---------\n",
      "Step  2 \n",
      "Discarded feature:  Sepal.Length \n",
      " Setting default kernel parameters  \n",
      "\n",
      "---------\n",
      "Step  3 \n",
      "Discarded feature:  Petal.Width \n",
      " Setting default kernel parameters  \n",
      "\n",
      "---------\n",
      "Step  4 \n",
      "Discarded feature:  Petal.Length \n",
      "\n",
      "---------\n",
      "Final ranking  4  features.\n",
      "Features:  Petal.Length Petal.Width Sepal.Length Sepal.Width \n"
     ]
    }
   ],
   "source": [
    "data(iris)\n",
    "\n",
    "cat(\"\\n-- Forward ranking random forest --\\n\")\n",
    "FORW.rf <- forward.ranking(iris[,-5],iris[,5],method=\"rf.est\" ,tot.trees=100,equalize.classes=F, verbosity=2)\n",
    "cat(\"\\n-- Forward ranking lda --\\n\")\n",
    "FORW.lda <- forward.ranking(iris[,-5],iris[,5],method=\"lda.est\", verbosity=2)\n",
    "\n",
    "cat(\"\\n-- Backward ranking random forest --\\n\")\n",
    "BACK.rf <- backward.ranking(iris[,-5],iris[,5],method=\"rf.est\" ,tot.trees=100,equalize.classes=F, verbosity=2)\n",
    "cat(\"\\n-- Backward ranking lda --\\n\")\n",
    "BACK.lda <- backward.ranking(iris[,-5],iris[,5],method=\"lda.est\", verbosity=2)\n",
    "\n",
    "cat(\"\\n-- Kruskal-Wallis --\\n\")\n",
    "KW <- kruskalwallis.ranking(iris[,-5], iris[,5], verbosity = 2)\n",
    "\n",
    "cat(\"\\n-- RFE linea random forest --\\n\")\n",
    "RFE.rf <- rfe.ranking(iris[,-5],iris[,5],method=\"imp.rf\", tot.trees=100, verbosity=2)\n",
    "cat(\"\\n-- RFE linear SVM --\\n\")\n",
    "RFE.linsvm <- rfe.ranking(iris[,-5],iris[,5],method=\"imp.linsvm\", verbosity=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
